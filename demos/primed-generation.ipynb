{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from gradio import Interface, Audio\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from functools import partial\n",
    "import gin\n",
    "import torchaudio\n",
    "from absl import app\n",
    "from torch.nn.functional import interpolate\n",
    "import pdb\n",
    "import logging\n",
    "import time\n",
    "import soundfile as sf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/mila/k/krishna-maneesha.dendukuri/.conda/envs/indian_improv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mila/k/krishna-maneesha.dendukuri\")\n",
    "sys.path.append(\"/home/mila/k/krishna-maneesha.dendukuri/GaMaDHaNi\")\n",
    "sys.path.append(\"/home/mila/k/krishna-maneesha.dendukuri/GaMaDHaNi/src\")\n",
    "import GaMaDHaNi\n",
    "from GaMaDHaNi import src, utils, scripts\n",
    "# from GaMaDHaNi.src import protobuf\n",
    "from src.protobuf.data_example import AudioExample\n",
    "from src.dataset import Task\n",
    "from utils.generate_utils import load_pitch_model, load_audio_model\n",
    "import utils.pitch_to_audio_utils as p2a\n",
    "from utils.utils import get_device, plot, save_figure, save_csv, save_audio\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "from generate import load_pitch_fns, load_audio_fns, generate_pitch, generate_audio\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "global_ind = -1\n",
    "global_audios = np.array([0.0])\n",
    "global_pitches = np.array([0])\n",
    "singer = 3\n",
    "audio_components = []\n",
    "preprocessed_primes = []\n",
    "selected_prime = None\n",
    "pitch_model_type = 'transformer'\n",
    "number_of_samples = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "pitch_path = '/home/mila/k/krishna-maneesha.dendukuri/scratch/checkpoints/msprior/x-transformer-debug/4340302'\n",
    "audio_path = '/network/scratch/n/nithya.shikarpur/checkpoints/pitch-diffusion/corrected-attention-v3/4835364'\n",
    "prime_file = '/home/mila/k/krishna-maneesha.dendukuri/notebooks/listening_study_primes.npz'\n",
    "db_path_audio = '/network/scratch/n/nithya.shikarpur/pitch-diffusion/data/merged_data-finalest/cached-audio-pitch-16k'\n",
    "output_folder = '/home/mila/k/krishna-maneesha.dendukuri/scratch/generations/demo_v1'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def generate(audio_model: str=None, \n",
    "            pitch_model: str=None, \n",
    "            pitch_model_type: str=None,\n",
    "            num_samples: int=1, \n",
    "            invert_pitch_fn=None,\n",
    "            invert_audio_fn=None,\n",
    "            num_steps: int=100, \n",
    "            singers: list=[3], \n",
    "            seq_len=1200,\n",
    "            temperature=1., \n",
    "            outfolder: str='temp', \n",
    "            audio_seq_len: int=750, \n",
    "            pitch_qt=None,\n",
    "            processed_primes=None, \n",
    "            prime=False,\n",
    "            ):\n",
    "    global global_ind, audio_components\n",
    "    global preprocessed_primes\n",
    "    global_ind += 1\n",
    "    logging.log(logging.INFO, 'Generate function')\n",
    "    pdb.set_trace()\n",
    "    pitch, inverted_pitch = generate_pitch(pitch_model=pitch_model,\n",
    "                                           pitch_model_type=pitch_model_type,\n",
    "                                           invert_pitch_fn=invert_pitch_fn, \n",
    "                                           num_samples=num_samples, \n",
    "                                           seq_len=seq_len,\n",
    "                                           temperature=temperature,\n",
    "                                           num_steps=num_steps, \n",
    "                                           outfolder=outfolder, \n",
    "                                           processed_primes=selected_prime if global_ind != 0 else None,\n",
    "                                           pitch_sample_rate=200,\n",
    "                                           prime=prime)\n",
    "    preprocessed_primes = pitch[:, :, 200:400]\n",
    "    if pitch_qt is not None:\n",
    "        def undo_qt(x, min_clip=200):\n",
    "            pitch= pitch_qt.inverse_transform(x.reshape(-1, 1)).reshape(1, -1)\n",
    "            pitch = np.around(pitch) # round to nearest integer, done in preprocessing of pitch contour fed into model\n",
    "            pitch[pitch < 200] = np.nan\n",
    "            return pitch\n",
    "        pitch = torch.tensor(np.array([undo_qt(x) for x in pitch.detach().cpu().numpy()])).to(pitch_model.device)\n",
    "    interpolated_pitch = p2a.interpolate_pitch(pitch=pitch, audio_seq_len=audio_seq_len)\n",
    "    interpolated_pitch = torch.nan_to_num(interpolated_pitch, nan=196)\n",
    "    interpolated_pitch = interpolated_pitch.squeeze(1) # to match input size by removing the extra dimension\n",
    "    audio = generate_audio(audio_model, \n",
    "                           interpolated_pitch, \n",
    "                           invert_audio_fn, \n",
    "                           singers=singers, \n",
    "                           num_steps=100, \n",
    "                           outfolder=outfolder)[:, :16000*4]\n",
    "    # pdb.set_trace()\n",
    "    audio = audio.detach().cpu().numpy()\n",
    "    state = [(16000, audio[0]), (16000, audio[1])]\n",
    "    pitch_vals = [np.where(inverted_pitch[0][:, 0] == 0, np.nan, inverted_pitch[0].flatten())[:400], np.where(inverted_pitch[1][:, 0] == 0, np.nan, inverted_pitch[1].flatten())[:400]]\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(pitch_vals[0], figure=fig1)\n",
    "    fig2 = plt.figure()\n",
    "    plt.plot(pitch_vals[1], figure=fig2)\n",
    "    state.append(fig1)\n",
    "    plt.close(fig1)\n",
    "    state.append(fig2)\n",
    "    plt.close(fig2)\n",
    "    state.append(state[0:2]) #this has to be fixed to be more elegant\n",
    "    state.append(pitch_vals)\n",
    "    return state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def clear_fn():\n",
    "    global global_ind, global_audios, global_pitches\n",
    "    global_ind = -1\n",
    "    global_audios = np.array([0.0])\n",
    "    global_pitches = np.array([0])\n",
    "    cmd = f\"rm -rf ./temp && mkdir ./temp\"\n",
    "    subprocess.run(cmd, shell=True)\n",
    "\n",
    "\n",
    "def update_choice(selected_options, full_audio, full_pitch, option, audiostate, pitchstate):\n",
    "    global global_ind\n",
    "    option = int(option)\n",
    "    selected_options.append({\"audio\": audiostate[option]})\n",
    "    full_audio[1] = np.concatenate((full_audio[1], audiostate[option][1][(2)*16000*int(global_ind!=0):]))\n",
    "    full_pitch = np.concatenate((full_pitch, pitchstate[option][200*int(global_ind!=0):]))\n",
    "    \n",
    "    global selected_prime, preprocessed_primes\n",
    "    selected_prime = preprocessed_primes[option].unsqueeze(0)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(full_pitch, figure=fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return (16000, full_audio[1]), fig, full_audio, full_pitch\n",
    "\n",
    "def save_session(full_pitch, full_audio):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    filename = f'session-{time.time()}'\n",
    "    logging.log(logging.INFO, f\"Saving session to {filename}\")\n",
    "    pd.DataFrame({'pitch': full_pitch, 'time': np.arange(0, len(full_pitch)/100, 0.01)}).to_csv(os.path.join(output_folder, filename + '.csv'), index=False)\n",
    "    sf.write(os.path.join(output_folder, filename + '.wav'), full_audio[1], 16000)\n",
    "\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        selected_options = gr.State([])\n",
    "        full_audio = gr.State((16000, np.array([])))\n",
    "        full_pitch = gr.State(np.array([]))\n",
    "        generated_audios = gr.State([(16000, []), (16000, [])])\n",
    "        generated_pitches = gr.State([None, None])\n",
    "\n",
    "        btn1 = gr.Button(\"Create Some Music!!\")\n",
    "        btn2 = gr.Button(\"Save Session\")\n",
    "        btn2.click(fn=save_session, inputs=[full_pitch, full_audio])\n",
    "\n",
    "        for i in range(10):\n",
    "            gr.Markdown(f\"### Generation {i+1}\")\n",
    "            audio_components.append(create_audio_components(visible=True))\n",
    "            audio_components[i]['submit'].click(fn=update_choice, \n",
    "                inputs=[selected_options, full_audio, full_pitch, audio_components[i]['dropdown'], generated_audios, generated_pitches], \n",
    "                outputs=[audio_components[i]['generated_audio'], audio_components[i]['generated_pitch'], full_audio, full_pitch])\n",
    "\n",
    "        for i in range(9):\n",
    "            audio_components[i]['generate_next'].click(fn=partial_generate, \n",
    "                inputs=[generated_audios, generated_pitches], \n",
    "                outputs=[audio_components[i+1]['option 0'], audio_components[i+1]['option 1'], \n",
    "                         audio_components[i+1]['pitch 0'], audio_components[i+1]['pitch 1'], \n",
    "                         generated_audios, generated_pitches])\n",
    "\n",
    "        btn1.click(fn=partial_generate, \n",
    "            inputs=[generated_audios, generated_pitches], \n",
    "            outputs=[audio_components[0]['option 0'], audio_components[0]['option 1'], \n",
    "                     audio_components[0]['pitch 0'], audio_components[0]['pitch 1'], \n",
    "                     generated_audios, generated_pitches])\n",
    "\n",
    "    return demo"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pitch model loaded on the device: cuda:0\n",
      "audio model loaded on the device: cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/mila/k/krishna-maneesha.dendukuri/.conda/envs/indian_improv/lib/python3.9/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator QuantileTransformer from version 1.2.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    pitch_model, pitch_qt, pitch_task_fn, invert_pitch_fn, primes = load_pitch_fns(pitch_path=pitch_path, \n",
    "                                                                                model_type = pitch_model_type, \n",
    "                                                                                prime=True, \n",
    "                                                                                prime_file=prime_file, \n",
    "                                                                                number_of_samples=number_of_samples)  \n",
    "    logging.info(f\"Pitch model loaded on the device: {pitch_model.device}\")\n",
    "    audio_model, audio_qt, audio_seq_len, invert_audio_fn = load_audio_fns(audio_path=audio_path, \n",
    "                                                                            db_path_audio=db_path_audio)\n",
    "    logging.info(f\"Audio model loaded on the device: {audio_model.device}\")\n",
    "    partial_generate = partial(generate, num_samples=2, num_steps=100, singers=[3], outfolder='temp', pitch_qt=pitch_qt)\n",
    "\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(show_example=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://f53ff0cfba68c02b44.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f53ff0cfba68c02b44.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1034456/3961911109.py\u001b[0m(22)\u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m    \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Generate function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m    pitch, inverted_pitch = generate_pitch(pitch_model=pitch_model,\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m                                           \u001b[0mpitch_model_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpitch_model_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m                                           \u001b[0minvert_pitch_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minvert_pitch_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import gradio\n",
    "gradio.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'4.41.0'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "indian_improv",
   "display_name": "indian_improv",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}