{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from gradio import Interface, Audio\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from functools import partial\n",
    "import gin\n",
    "import torchaudio\n",
    "from absl import app\n",
    "from torch.nn.functional import interpolate\n",
    "import pdb\n",
    "import logging\n",
    "import time\n",
    "import soundfile as sf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/mila/k/krishna-maneesha.dendukuri/.conda/envs/indian_improv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mila/k/krishna-maneesha.dendukuri\")\n",
    "sys.path.append(\"/home/mila/k/krishna-maneesha.dendukuri/GaMaDHaNi\")\n",
    "sys.path.append(\"/home/mila/k/krishna-maneesha.dendukuri/GaMaDHaNi/src\")\n",
    "import GaMaDHaNi\n",
    "from GaMaDHaNi import src, utils, scripts\n",
    "# from GaMaDHaNi.src import protobuf\n",
    "from src.protobuf.data_example import AudioExample\n",
    "from src.dataset import Task\n",
    "from utils.generate_utils import load_pitch_model, load_audio_model\n",
    "import utils.pitch_to_audio_utils as p2a\n",
    "from utils.utils import get_device, plot, save_figure, save_csv, save_audio\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "from generate import load_pitch_fns, load_audio_fns, generate_pitch, generate_audio\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "global_ind = -1\n",
    "global_audios = np.array([0.0])\n",
    "global_pitches = np.array([0])\n",
    "singer = 3\n",
    "audio_components = []\n",
    "preprocessed_primes = []\n",
    "selected_prime = None\n",
    "pitch_model_type = 'transformer'\n",
    "number_of_samples = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "pitch_path = '/home/mila/k/krishna-maneesha.dendukuri/scratch/checkpoints/msprior/x-transformer-debug/4340302'\n",
    "audio_path = '/network/scratch/n/nithya.shikarpur/checkpoints/pitch-diffusion/corrected-attention-v3/4835364'\n",
    "prime_file = '/home/mila/k/krishna-maneesha.dendukuri/notebooks/listening_study_primes.npz'\n",
    "db_path_audio = '/network/scratch/n/nithya.shikarpur/pitch-diffusion/data/merged_data-finalest/cached-audio-pitch-16k'\n",
    "output_folder = '/home/mila/k/krishna-maneesha.dendukuri/scratch/generations/demo_v1'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def generate(audio_model: str=None, \n",
    "            pitch_model: str=None, \n",
    "            pitch_model_type: str=None,\n",
    "            num_samples: int=1, \n",
    "            invert_pitch_fn=None,\n",
    "            invert_audio_fn=None,\n",
    "            num_steps: int=100, \n",
    "            singers: list=[3], \n",
    "            seq_len=1200,\n",
    "            temperature=1., \n",
    "            outfolder: str='temp', \n",
    "            audio_seq_len: int=750, \n",
    "            pitch_qt=None,\n",
    "            processed_primes=None, \n",
    "            prime=False,\n",
    "            ):\n",
    "    global global_ind, audio_components\n",
    "    global preprocessed_primes\n",
    "    global_ind += 1\n",
    "    logging.log(logging.INFO, 'Generate function')\n",
    "    pdb.set_trace()\n",
    "    pitch, inverted_pitch = generate_pitch(pitch_model=pitch_model,\n",
    "                                           pitch_model_type=pitch_model_type,\n",
    "                                           invert_pitch_fn=invert_pitch_fn, \n",
    "                                           num_samples=num_samples, \n",
    "                                           seq_len=seq_len,\n",
    "                                           temperature=temperature,\n",
    "                                           num_steps=num_steps, \n",
    "                                           outfolder=outfolder, \n",
    "                                           processed_primes=selected_prime if global_ind != 0 else None,\n",
    "                                           pitch_sample_rate=200,\n",
    "                                           prime=prime)\n",
    "    preprocessed_primes = pitch[:, :, 200:400]\n",
    "    if pitch_qt is not None:\n",
    "        def undo_qt(x, min_clip=200):\n",
    "            pitch= pitch_qt.inverse_transform(x.reshape(-1, 1)).reshape(1, -1)\n",
    "            pitch = np.around(pitch) # round to nearest integer, done in preprocessing of pitch contour fed into model\n",
    "            pitch[pitch < 200] = np.nan\n",
    "            return pitch\n",
    "        pitch = torch.tensor(np.array([undo_qt(x) for x in pitch.detach().cpu().numpy()])).to(pitch_model.device)\n",
    "    interpolated_pitch = p2a.interpolate_pitch(pitch=pitch, audio_seq_len=audio_seq_len)\n",
    "    interpolated_pitch = torch.nan_to_num(interpolated_pitch, nan=196)\n",
    "    interpolated_pitch = interpolated_pitch.squeeze(1) # to match input size by removing the extra dimension\n",
    "    audio = generate_audio(audio_model, \n",
    "                           interpolated_pitch, \n",
    "                           invert_audio_fn, \n",
    "                           singers=singers, \n",
    "                           num_steps=100, \n",
    "                           outfolder=outfolder)[:, :16000*4]\n",
    "    # pdb.set_trace()\n",
    "    audio = audio.detach().cpu().numpy()\n",
    "    state = [(16000, audio[0]), (16000, audio[1])]\n",
    "    pitch_vals = [np.where(inverted_pitch[0][:, 0] == 0, np.nan, inverted_pitch[0].flatten())[:400], np.where(inverted_pitch[1][:, 0] == 0, np.nan, inverted_pitch[1].flatten())[:400]]\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(pitch_vals[0], figure=fig1)\n",
    "    fig2 = plt.figure()\n",
    "    plt.plot(pitch_vals[1], figure=fig2)\n",
    "    state.append(fig1)\n",
    "    plt.close(fig1)\n",
    "    state.append(fig2)\n",
    "    plt.close(fig2)\n",
    "    state.append(state[0:2]) #this has to be fixed to be more elegant\n",
    "    state.append(pitch_vals)\n",
    "    return state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def clear_fn():\n",
    "    global global_ind, global_audios, global_pitches\n",
    "    global_ind = -1\n",
    "    global_audios = np.array([0.0])\n",
    "    global_pitches = np.array([0])\n",
    "    cmd = f\"rm -rf ./temp && mkdir ./temp\"\n",
    "    subprocess.run(cmd, shell=True)\n",
    "\n",
    "\n",
    "def update_choice(selected_options, full_audio, full_pitch, option, audiostate, pitchstate):\n",
    "    global global_ind\n",
    "    option = int(option)\n",
    "    selected_options.append({\"audio\": audiostate[option]})\n",
    "    full_audio[1] = np.concatenate((full_audio[1], audiostate[option][1][(2)*16000*int(global_ind!=0):]))\n",
    "    full_pitch = np.concatenate((full_pitch, pitchstate[option][200*int(global_ind!=0):]))\n",
    "    \n",
    "    global selected_prime, preprocessed_primes\n",
    "    selected_prime = preprocessed_primes[option].unsqueeze(0)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(full_pitch, figure=fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return (16000, full_audio[1]), fig, full_audio, full_pitch\n",
    "\n",
    "def save_session(full_pitch, full_audio):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    filename = f'session-{time.time()}'\n",
    "    logging.log(logging.INFO, f\"Saving session to {filename}\")\n",
    "    pd.DataFrame({'pitch': full_pitch, 'time': np.arange(0, len(full_pitch)/100, 0.01)}).to_csv(os.path.join(output_folder, filename + '.csv'), index=False)\n",
    "    sf.write(os.path.join(output_folder, filename + '.wav'), full_audio[1], 16000)\n",
    "\n",
    "pitch_model, pitch_qt, pitch_task_fn, invert_pitch_fn, primes = load_pitch_fns(pitch_path=pitch_path, \n",
    "                                                                                model_type = pitch_model_type, \n",
    "                                                                                prime=True, \n",
    "                                                                                prime_file=prime_file, \n",
    "                                                                                number_of_samples=number_of_samples)  \n",
    "print(f\"pitch model loaded on the device: {pitch_model.device}\")\n",
    "audio_model, audio_qt, audio_seq_len, invert_audio_fn = load_audio_fns(audio_path=audio_path, \n",
    "                                                                        db_path_audio=db_path_audio)\n",
    "print(f\"audio model loaded on the device: {audio_model.device}\")\n",
    "partial_generate = partial(generate, num_samples=2, num_steps=100, singers=[3], outfolder='temp', pitch_qt=pitch_qt)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    selected_options = gr.State([])\n",
    "    full_audio = gr.State((16000, np.array([])))\n",
    "    full_pitch = gr.State(np.array([]))\n",
    "    generated_audios = gr.State([(16000, []), (16000, [])])\n",
    "    generated_pitches = gr.State([None, None])\n",
    "    def add_components(visible=False):\n",
    "        # Generation 1\n",
    "        with gr.Row(visible=visible):\n",
    "            o1 = gr.Audio( label=\"Option 0\")\n",
    "            o2 = gr.Audio(label=\"Option 1\")\n",
    "        with gr.Row():\n",
    "            img1 = gr.Plot(label=\"Option 0\")\n",
    "            img2 = gr.Plot(label=\"Option 1\")\n",
    "        dd1 = gr.Dropdown(label=\"Select the best audio\", choices=[\"0\", \"1\"], interactive=True, visible=visible)\n",
    "        sbmtbtn1 = gr.Button(\"Submit\", visible=visible)\n",
    "        gen_audio = gr.Audio(label=\"Generated Audio\", visible=visible)\n",
    "        gen_pitch = gr.Plot(label=\"Generated Pitch\", visible=visible)\n",
    "        with gr.Row(visible=visible):\n",
    "            # redo = gr.Button(\"Redo (doesn't work yet)\")\n",
    "            # redo.click(fn=partial_generate, inputs=[generated_audios], outputs=[o1, o2, generated_audios])\n",
    "            generate_next = gr.Button(\"Generate Next\")\n",
    "\n",
    "\n",
    "        return {\n",
    "            'option 0': o1,\n",
    "            'option 1': o2,\n",
    "            'pitch 0': img1,\n",
    "            'pitch 1': img2,\n",
    "            'dropdown': dd1,\n",
    "            'submit': sbmtbtn1,\n",
    "            'generated_audio': gen_audio,\n",
    "            'generated_pitch': gen_pitch,\n",
    "            # 'redo': redo,\n",
    "            'generate_next': generate_next\n",
    "        }\n",
    "\n",
    "    btn1 = gr.Button(\"Create Some Music!!\")\n",
    "    btn2 = gr.Button(\"Save Session\")\n",
    "    btn2.click(fn=save_session, inputs=[full_pitch, full_audio])\n",
    "    for i in range(10):\n",
    "        gr.Markdown(f\"### Generation {i+1}\")\n",
    "        audio_components.append(add_components(visible=True))\n",
    "        audio_components[i]['submit'].click(fn=update_choice, inputs=[selected_options, full_audio, full_pitch, audio_components[i]['dropdown'], generated_audios, generated_pitches], outputs=[audio_components[i]['generated_audio'], audio_components[i]['generated_pitch'], full_audio, full_pitch])\n",
    "    for i in range(9):\n",
    "        audio_components[i]['generate_next'].click(fn=partial_generate, inputs=[generated_audios, generated_pitches], outputs=[audio_components[i+1]['option 0'], audio_components[i+1]['option 1'], audio_components[i+1]['pitch 0'], audio_components[i+1]['pitch 1'], generated_audios, generated_pitches])\n",
    "    btn1.click(fn=partial_generate, inputs=[generated_audios, generated_pitches], outputs=[audio_components[0]['option 0'], audio_components[0]['option 1'], audio_components[0]['pitch 0'], audio_components[0]['pitch 1'], generated_audios, generated_pitches])\n",
    "    \n",
    "    # sbmtbtn.click(fn=update_choice, inputs=[selected_options, full_audio, full_pitch, dd1, [o1, o2], [o1, o2]])\n",
    "    # Generation 1\n",
    "    # gr.Markdown(\"### Generate Audio!!!\")\n",
    "    # with gr.Row():\n",
    "    #     o1 = gr.Audio(label=\"Option 0\")\n",
    "    #     o2 = gr.Audio(label=\"Option 1\")\n",
    "    # # with gr.Row():\n",
    "    # #     img1 = gr.Image(label=\"Option 0\")\n",
    "    # #     img2 = gr.Image(label=\"Option 1\")\n",
    "    # btn1 = gr.Button(\"Run\")\n",
    "    # btn1.click(fn=partial_generate, inputs=None, outputs=[o1, o2])\n",
    "    # btn2 = gr.Button(\"Add Pair\")\n",
    "    # btn2.click(fn=add_pair(state))\n",
    "    # dd1 = gr.Dropdown(label=\"Select the best audio\", choices=[\"0\", \"1\"], interactive=True)\n",
    "    # sbmtbtn1 = gr.Button(\"Submit\")\n",
    "def main(argv):\n",
    "    \n",
    "    demo.launch(show_example=False)[0].children[-1].click = add_pair.partial(state)\n",
    "    # demo.launch(share=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pitch model loaded on the device: cuda:0\n",
      "audio model loaded on the device: cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "indian_improv",
   "display_name": "indian_improv",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}